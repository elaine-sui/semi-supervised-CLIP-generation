{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import sys\n",
    "sys.path.append('/pasteur/u/esui/repos/semi-supervised-CLIP-generation/')\n",
    "\n",
    "from eval_new_gens import run_eval"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/pasteur/u/esui/miniconda3/envs/clip_prefix_caption2/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package wordnet to /sailhome/esui/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /sailhome/esui/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /sailhome/esui/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "dir = \"reformatted_from_new_repo_imagebind_llama\"\n",
    "os.makedirs(dir, exist_ok=True)\n",
    "\n",
    "raw_pred_dir = \"/pasteur/u/yuhuiz/iccv_c3/cache/generation_data\"\n",
    "full_refs_path = \"/pasteur/u/yuhuiz/iccv_c3/data/data_image_coco_imagebind.pkl\"\n",
    "\n",
    "all_files = glob(f'{raw_pred_dir}/*')\n",
    "\n",
    "skip_list = [f\"/pasteur/u/yuhuiz/iccv_c3/cache/generation_data/data_image_coco_imagebind.pkl_c1_0.1_{i}.json\" for i in range(1, 6)]\n",
    "\n",
    "for raw_pred_file in sorted(all_files):\n",
    "    if raw_pred_file in skip_list:\n",
    "        continue\n",
    "\n",
    "    print(raw_pred_file)\n",
    "    \n",
    "    filename = os.path.splitext(os.path.split(raw_pred_file)[1])[0]\n",
    "    out_file = f'{dir}/{filename}_metrics.json'\n",
    "\n",
    "    run_eval(raw_pred_file, out_file=out_file, dir=dir, full_refs_path=full_refs_path)\n",
    "    \n",
    "    print(\"=\"*80)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/pasteur/u/yuhuiz/iccv_c3/cache/generation_data/data_image_coco_imagebind.pkl_c1_0.1_1.json\n",
      "=> New pred file: reformatted_from_new_repo_imagebind_llama/data_image_coco_imagebind.pkl_c1_0.1_1_converted_preds.json\n",
      "loading annotations into memory...\n",
      "Done (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "PTBTokenizer tokenized 307125 tokens at 867807.86 tokens per second.\n",
      "PTBTokenizer tokenized 74220 tokens at 388440.44 tokens per second.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 66285, 'reflen': 56760, 'guess': [66285, 61309, 56333, 51357], 'correct': [29755, 9541, 2778, 871]}\n",
      "ratio: 1.1678118393234467\n",
      "Bleu_1: 0.449\n",
      "Bleu_2: 0.264\n",
      "Bleu_3: 0.151\n",
      "Bleu_4: 0.087\n",
      "computing METEOR score...\n",
      "METEOR: 0.163\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.341\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.336\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "SPICE evaluation took: 6.462 s\n",
      "SPICE: 0.106\n",
      "{'Bleu_1': 0.44889492343666815, 'Bleu_2': 0.2643060952605215, 'Bleu_3': 0.15102931044859164, 'Bleu_4': 0.08742800264836573, 'METEOR': 0.16332889726199684, 'ROUGE_L': 0.34128139443737665, 'CIDEr': 0.33614727054932425, 'SPICE': 0.10554541465087239}\n",
      "=> Metrics file at reformatted_from_new_repo_imagebind_llama/data_image_coco_imagebind.pkl_c1_0.1_1_metrics.json\n",
      "================================================================================\n",
      "/pasteur/u/yuhuiz/iccv_c3/cache/generation_data/data_image_coco_imagebind.pkl_c1_0.1_2.json\n",
      "=> New pred file: reformatted_from_new_repo_imagebind_llama/data_image_coco_imagebind.pkl_c1_0.1_2_converted_preds.json\n",
      "loading annotations into memory...\n",
      "Done (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "PTBTokenizer tokenized 307125 tokens at 889642.79 tokens per second.\n",
      "PTBTokenizer tokenized 65609 tokens at 373658.89 tokens per second.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 58339, 'reflen': 53353, 'guess': [58339, 53408, 48478, 43548], 'correct': [26215, 7818, 2180, 645]}\n",
      "ratio: 1.0934530391917776\n",
      "Bleu_1: 0.449\n",
      "Bleu_2: 0.256\n",
      "Bleu_3: 0.144\n",
      "Bleu_4: 0.081\n",
      "computing METEOR score...\n",
      "METEOR: 0.150\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.321\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.335\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "SPICE evaluation took: 6.934 s\n",
      "SPICE: 0.099\n",
      "{'Bleu_1': 0.44935634824044896, 'Bleu_2': 0.2564720931978187, 'Bleu_3': 0.14354806542714343, 'Bleu_4': 0.08135717932413067, 'METEOR': 0.1502252333590469, 'ROUGE_L': 0.32114102464808225, 'CIDEr': 0.33543247120664765, 'SPICE': 0.09879277894010174}\n",
      "=> Metrics file at reformatted_from_new_repo_imagebind_llama/data_image_coco_imagebind.pkl_c1_0.1_2_metrics.json\n",
      "================================================================================\n",
      "/pasteur/u/yuhuiz/iccv_c3/cache/generation_data/data_image_coco_imagebind.pkl_c1_0.1_3.json\n",
      "=> New pred file: reformatted_from_new_repo_imagebind_llama/data_image_coco_imagebind.pkl_c1_0.1_3_converted_preds.json\n",
      "loading annotations into memory...\n",
      "Done (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "PTBTokenizer tokenized 307125 tokens at 900861.66 tokens per second.\n",
      "Sep 28, 2023 12:24:32 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: ? (U+D83D, decimal: 55357)\n",
      "PTBTokenizer tokenized 71187 tokens at 305461.04 tokens per second.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 63520, 'reflen': 54921, 'guess': [63520, 58571, 53622, 48673], 'correct': [27267, 8176, 2254, 658]}\n",
      "ratio: 1.1565703464976758\n",
      "Bleu_1: 0.429\n",
      "Bleu_2: 0.245\n",
      "Bleu_3: 0.136\n",
      "Bleu_4: 0.076\n",
      "computing METEOR score...\n",
      "METEOR: 0.153\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.321\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.334\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.5 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [0.9 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.5 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.5 sec].\n",
      "Threads( StanfordCoreNLP ) [04:40.419 minutes]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "SPICE evaluation took: 4.898 min\n",
      "SPICE: 0.102\n",
      "{'Bleu_1': 0.429266372795963, 'Bleu_2': 0.2447893709501339, 'Bleu_3': 0.13606048206007348, 'Bleu_4': 0.07638946151491408, 'METEOR': 0.15294747090326297, 'ROUGE_L': 0.3208704461413581, 'CIDEr': 0.3342396003637391, 'SPICE': 0.10176309759844007}\n",
      "=> Metrics file at reformatted_from_new_repo_imagebind_llama/data_image_coco_imagebind.pkl_c1_0.1_3_metrics.json\n",
      "================================================================================\n",
      "/pasteur/u/yuhuiz/iccv_c3/cache/generation_data/data_image_coco_imagebind.pkl_c1_0.1_4.json\n",
      "=> New pred file: reformatted_from_new_repo_imagebind_llama/data_image_coco_imagebind.pkl_c1_0.1_4_converted_preds.json\n",
      "loading annotations into memory...\n",
      "Done (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "PTBTokenizer tokenized 307125 tokens at 932942.08 tokens per second.\n",
      "Sep 28, 2023 12:29:55 PM edu.stanford.nlp.process.PTBLexer next\n",
      "WARNING: Untokenizable: ? (U+D83D, decimal: 55357)\n",
      "PTBTokenizer tokenized 67063 tokens at 351796.14 tokens per second.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 59844, 'reflen': 53524, 'guess': [59844, 54955, 50068, 45181], 'correct': [25931, 7440, 1980, 546]}\n",
      "ratio: 1.118077871608977\n",
      "Bleu_1: 0.433\n",
      "Bleu_2: 0.242\n",
      "Bleu_3: 0.132\n",
      "Bleu_4: 0.073\n",
      "computing METEOR score...\n",
      "METEOR: 0.148\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.314\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.316\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.6 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.2 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.5 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.5 sec].\n",
      "Threads( StanfordCoreNLP ) [03:55.547 minutes]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "SPICE evaluation took: 4.133 min\n",
      "SPICE: 0.099\n",
      "{'Bleu_1': 0.4333099391751816, 'Bleu_2': 0.24220448847668946, 'Bleu_3': 0.13238022258665905, 'Bleu_4': 0.07276566977099466, 'METEOR': 0.14798823241243506, 'ROUGE_L': 0.31415200860155573, 'CIDEr': 0.31585635735550616, 'SPICE': 0.0990009439947974}\n",
      "=> Metrics file at reformatted_from_new_repo_imagebind_llama/data_image_coco_imagebind.pkl_c1_0.1_4_metrics.json\n",
      "================================================================================\n",
      "/pasteur/u/yuhuiz/iccv_c3/cache/generation_data/data_image_coco_imagebind.pkl_c1_0.1_5.json\n",
      "=> New pred file: reformatted_from_new_repo_imagebind_llama/data_image_coco_imagebind.pkl_c1_0.1_5_converted_preds.json\n",
      "loading annotations into memory...\n",
      "Done (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "PTBTokenizer tokenized 307125 tokens at 796439.85 tokens per second.\n",
      "PTBTokenizer tokenized 67414 tokens at 339689.31 tokens per second.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 59556, 'reflen': 53513, 'guess': [59556, 54634, 49714, 44794], 'correct': [25288, 7050, 1816, 496]}\n",
      "ratio: 1.1129258311064394\n",
      "Bleu_1: 0.425\n",
      "Bleu_2: 0.234\n",
      "Bleu_3: 0.126\n",
      "Bleu_4: 0.069\n",
      "computing METEOR score...\n",
      "METEOR: 0.142\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.308\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.286\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.5 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.2 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.5 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.5 sec].\n",
      "Threads( StanfordCoreNLP ) [03:43.448 minutes]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "SPICE evaluation took: 3.929 min\n",
      "SPICE: 0.090\n",
      "{'Bleu_1': 0.42460877157632443, 'Bleu_2': 0.2340763517974168, 'Bleu_3': 0.1260232684039303, 'Bleu_4': 0.06861254234531991, 'METEOR': 0.14164343658661369, 'ROUGE_L': 0.30825195684196777, 'CIDEr': 0.2856997812633969, 'SPICE': 0.09021545537325694}\n",
      "=> Metrics file at reformatted_from_new_repo_imagebind_llama/data_image_coco_imagebind.pkl_c1_0.1_5_metrics.json\n",
      "================================================================================\n",
      "/pasteur/u/yuhuiz/iccv_c3/cache/generation_data/data_image_coco_imagebind.pkl_c1_0.1_6.json\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "list index out of range",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplitext(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplit(raw_pred_file)[\u001b[38;5;241m1\u001b[39m])[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     13\u001b[0m out_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mdir\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_metrics.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 15\u001b[0m \u001b[43mrun_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_pred_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdir\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mdir\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_refs_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfull_refs_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m80\u001b[39m)\n",
      "File \u001b[0;32m/pasteur/u/esui/repos/semi-supervised-CLIP-generation/src/eval/eval_new_gens.py:79\u001b[0m, in \u001b[0;36mrun_eval\u001b[0;34m(raw_pred_file, out_file, dir, full_refs_path)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_eval\u001b[39m(raw_pred_file, out_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mdir\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, full_refs_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 79\u001b[0m     new_pred_file, new_labels_file \u001b[38;5;241m=\u001b[39m \u001b[43mreformat_preds\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_pred_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdir\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mdir\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_refs_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfull_refs_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m     metrics_dict \u001b[38;5;241m=\u001b[39m evaluate_on_coco_caption(new_pred_file, new_labels_file, out_file)\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28mprint\u001b[39m(metrics_dict)\n",
      "File \u001b[0;32m/pasteur/u/esui/repos/semi-supervised-CLIP-generation/src/eval/eval_new_gens.py:31\u001b[0m, in \u001b[0;36mreformat_preds\u001b[0;34m(raw_pred_file, dir, full_refs_path)\u001b[0m\n\u001b[1;32m     29\u001b[0m     gen \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m eos \u001b[38;5;129;01min\u001b[39;00m gen:\n\u001b[0;32m---> 31\u001b[0m     gen \u001b[38;5;241m=\u001b[39m \u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfindall\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m<s>(.*?)</s>\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m     gen \u001b[38;5;241m=\u001b[39m gen\u001b[38;5;241m.\u001b[39mreplace(sos, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "raw_pred_file = \"/pasteur/u/yuhuiz/iccv_c3/cache/generation_data/data_image_coco_imagebind.pkl_c1_0.1_3.json\"\n",
    "\n",
    "# Load generations\n",
    "with open(raw_pred_file, \"r\") as f:\n",
    "    gens, _ = json.load(f)\n",
    "gens = gens[0]\n",
    "\n",
    "# Clean generations\n",
    "sos = \"<s>\"\n",
    "eos = \"</s>\"\n",
    "\n",
    "processed_gens = []\n",
    "for gen in gens:\n",
    "    print(f\"Raw: {gen}\")\n",
    "    if gen[:len(sos)] != sos:\n",
    "        gen = \"\"\n",
    "    elif eos in gen:\n",
    "        gen = re.findall(r\"<s>(.*?)</s>\", gen)[0].strip()\n",
    "    else:\n",
    "        gen = gen.replace(sos, \"\").strip()\n",
    "\n",
    "    print(f\"Processed: {gen}\")\n",
    "    processed_gens.append(gen)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.16",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.16 64-bit ('clip_prefix_caption2': conda)"
  },
  "interpreter": {
   "hash": "b9a897870f89d72e3f0e339b8f347f274cca8b2d5960bd08ef4da7a9cc09c88a"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}