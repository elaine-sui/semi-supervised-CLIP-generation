{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import argparse\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.append('/pasteur/u/yuhuiz/archive/neurips_modality_gap/pull_figure/convirt/')\n",
    "from data.pretrain_loader import PretrainDataset"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Load model\n",
    "from medclip import MedCLIPModel, MedCLIPVisionModelViT\n",
    "\n",
    "# load MedCLIP-ViT\n",
    "model = MedCLIPModel(vision_cls=MedCLIPVisionModelViT)\n",
    "model.from_pretrained(input_dir='/pasteur/u/esui/data/medclip_pretrained')\n",
    "model.to('cuda')\n",
    "\n",
    "# vision_model = model.vision_model\n",
    "# text_model = model.text_model"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/pasteur/u/esui/miniconda3/envs/medclip/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading: 100%|██████████| 71.8k/71.8k [00:00<00:00, 1.15MB/s]\n",
      "Downloading: 100%|██████████| 113M/113M [00:01<00:00, 109MB/s]\n",
      "Some weights of the model checkpoint at microsoft/swin-tiny-patch4-window7-224 were not used when initializing SwinModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing SwinModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing SwinModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Downloading: 100%|██████████| 385/385 [00:00<00:00, 297kB/s]\n",
      "Downloading: 100%|██████████| 436M/436M [00:03<00:00, 128MB/s]\n",
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Downloading: 100%|██████████| 213k/213k [00:00<00:00, 1.62MB/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "load model weight from: /pasteur/u/esui/data/medclip_pretrained\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "MedCLIPModel(\n",
       "  (vision_model): MedCLIPVisionModelViT(\n",
       "    (model): SwinModel(\n",
       "      (embeddings): SwinEmbeddings(\n",
       "        (patch_embeddings): SwinPatchEmbeddings(\n",
       "          (projection): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
       "        )\n",
       "        (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (encoder): SwinEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): SwinStage(\n",
       "            (blocks): ModuleList(\n",
       "              (0-1): 2 x SwinLayer(\n",
       "                (layernorm_before): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "                (attention): SwinAttention(\n",
       "                  (self): SwinSelfAttention(\n",
       "                    (query): Linear(in_features=96, out_features=96, bias=True)\n",
       "                    (key): Linear(in_features=96, out_features=96, bias=True)\n",
       "                    (value): Linear(in_features=96, out_features=96, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                  (output): SwinSelfOutput(\n",
       "                    (dense): Linear(in_features=96, out_features=96, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (drop_path): SwinDropPath(p=0.1)\n",
       "                (layernorm_after): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "                (intermediate): SwinIntermediate(\n",
       "                  (dense): Linear(in_features=96, out_features=384, bias=True)\n",
       "                  (intermediate_act_fn): GELUActivation()\n",
       "                )\n",
       "                (output): SwinOutput(\n",
       "                  (dense): Linear(in_features=384, out_features=96, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (downsample): SwinPatchMerging(\n",
       "              (reduction): Linear(in_features=384, out_features=192, bias=False)\n",
       "              (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (1): SwinStage(\n",
       "            (blocks): ModuleList(\n",
       "              (0-1): 2 x SwinLayer(\n",
       "                (layernorm_before): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                (attention): SwinAttention(\n",
       "                  (self): SwinSelfAttention(\n",
       "                    (query): Linear(in_features=192, out_features=192, bias=True)\n",
       "                    (key): Linear(in_features=192, out_features=192, bias=True)\n",
       "                    (value): Linear(in_features=192, out_features=192, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                  (output): SwinSelfOutput(\n",
       "                    (dense): Linear(in_features=192, out_features=192, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (drop_path): SwinDropPath(p=0.1)\n",
       "                (layernorm_after): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                (intermediate): SwinIntermediate(\n",
       "                  (dense): Linear(in_features=192, out_features=768, bias=True)\n",
       "                  (intermediate_act_fn): GELUActivation()\n",
       "                )\n",
       "                (output): SwinOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=192, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (downsample): SwinPatchMerging(\n",
       "              (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
       "              (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (2): SwinStage(\n",
       "            (blocks): ModuleList(\n",
       "              (0-5): 6 x SwinLayer(\n",
       "                (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "                (attention): SwinAttention(\n",
       "                  (self): SwinSelfAttention(\n",
       "                    (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "                    (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "                    (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                  (output): SwinSelfOutput(\n",
       "                    (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (drop_path): SwinDropPath(p=0.1)\n",
       "                (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "                (intermediate): SwinIntermediate(\n",
       "                  (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                  (intermediate_act_fn): GELUActivation()\n",
       "                )\n",
       "                (output): SwinOutput(\n",
       "                  (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (downsample): SwinPatchMerging(\n",
       "              (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
       "              (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (3): SwinStage(\n",
       "            (blocks): ModuleList(\n",
       "              (0-1): 2 x SwinLayer(\n",
       "                (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (attention): SwinAttention(\n",
       "                  (self): SwinSelfAttention(\n",
       "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                  (output): SwinSelfOutput(\n",
       "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (drop_path): SwinDropPath(p=0.1)\n",
       "                (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (intermediate): SwinIntermediate(\n",
       "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                  (intermediate_act_fn): GELUActivation()\n",
       "                )\n",
       "                (output): SwinOutput(\n",
       "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (pooler): AdaptiveAvgPool1d(output_size=1)\n",
       "    )\n",
       "    (projection_head): Linear(in_features=768, out_features=512, bias=False)\n",
       "  )\n",
       "  (text_model): MedCLIPTextModel(\n",
       "    (model): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): BertPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (projection_head): Linear(in_features=768, out_features=512, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "base_dir = \"/u/scr/zyh/develop/data-open/mimic-cxr-jpg-resized\"\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--data_dir', type=str, help=\"Data directory with train and valid indexed report files.\", default=None)\n",
    "    parser.add_argument('--meta_file', type=str, default=f'{base_dir}/meta.json', help=\"Dataset meta file.\")\n",
    "    parser.add_argument('--img_dir', type=str, default=f'{base_dir}/files/', help=\"Directory to load image data from.\")\n",
    "    parser.add_argument('--local_img_dir', type=str, default=f'{base_dir}/files/', help=\"Directory to load image data from.\")\n",
    "    parser.add_argument('--image_encoder', type=str, default='resnet50', help=\"Name of the model architecture.\")\n",
    "    parser.add_argument('--bert_name', type=str, default='emilyalsentzer/Bio_ClinicalBERT', help=\"Name of the pretrained BERT model.\")\n",
    "    parser.add_argument('--imsize', type=int, default=224, help=\"Size of image.\")\n",
    "    parser.add_argument('--augment_p', type=float, default=0., help=\"Probability for image augmentation.\")\n",
    "    parser.add_argument('--dropout', type=float, default=0.2, help=\"Dropout rate.\")\n",
    "    parser.add_argument('--finetune_text_encoder', dest='freeze_text_encoder', action='store_false', help=\"Whether to finetune text encoder.\")\n",
    "\n",
    "    parser.add_argument('--num_clf_layer', type=int, default=2, help=\"Number of layers to use for NN classifier.\")\n",
    "    parser.add_argument('--clf_hidden_dim', type=int, default=512, help=\"Number of hidden dims for NN classifier.\")\n",
    "    parser.add_argument('--pool', choices=['cls', 'mean', 'max'], default='mean', help=\"Type of pooling to use for text encoder.\")\n",
    "\n",
    "    parser.add_argument('--fp', dest='amp', action='store_false', help=\"Use full precision training; by default use mixed precision.\")\n",
    "    parser.add_argument('--rih', action='store_true', help=\"Train on the RIH data; use corresponding data loaders.\")\n",
    "\n",
    "    parser.add_argument('--batch_size', type=int, default=32)\n",
    "    parser.add_argument('--num_epoch', type=int, default=200)\n",
    "    parser.add_argument('--steps_per_epoch', type=int, default=5000)\n",
    "    parser.add_argument('--optim', type=str, default='adam', help='sgd, adagrad, adam, adamw or adamax.')\n",
    "    parser.add_argument('--lr', type=float, default=1e-4)\n",
    "    parser.add_argument('--weight_decay', type=float, default=1e-6)\n",
    "    parser.add_argument('--patience', type=int, default=5)\n",
    "    parser.add_argument('--annealing_factor', type=float, default=0.5)\n",
    "    parser.add_argument('--log_interval', type=int, default=100)\n",
    "    parser.add_argument('--num_workers', type=int, default=4)\n",
    "    parser.add_argument('--pin_memory', action='store_true')\n",
    "    parser.add_argument('--save_dir', type=str, default=None, help=\"Directory to save the trained model; if None will use id to look up\")\n",
    "    parser.add_argument('--root_dir', type=str, default='saved_models/pretrain', help=\"Root directory for model saving.\")\n",
    "    parser.add_argument('--id', type=int, default=0, help=\"An id of the training run\")\n",
    "    parser.add_argument('--seed', type=int, default=1234)\n",
    "\n",
    "    args = parser.parse_args(\"\")\n",
    "    return args\n",
    "\n",
    "args = parse_args()\n",
    "opt = vars(args)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "tokenizer = model.text_model.tokenizer #AutoTokenizer.from_pretrained(opt['bert_name'])\n",
    "\n",
    "dataset = PretrainDataset(\n",
    "    indexed_file=f\"{base_dir}/reports_indexed.json\",\n",
    "    meta_file=f\"{base_dir}/meta.json\",\n",
    "    img_dir=f\"{base_dir}/files/\",\n",
    "    opt=opt,\n",
    "    tokenizer=tokenizer,\n",
    "    evaluation=False,\n",
    "    imsize=opt['imsize'],\n",
    "    augment_p=opt['augment_p']\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# random sample 500 idxs from dataset\n",
    "random.seed(1234)\n",
    "idxs = random.sample(range(len(dataset)), 10000)\n",
    "all_img_v, all_text_v = [], []\n",
    "\n",
    "data = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx in tqdm(idxs):\n",
    "        image, text_ids = dataset[idx]\n",
    "        \n",
    "        image = torch.tensor(image).unsqueeze(0).cuda()\n",
    "        text_ids = torch.tensor(text_ids).unsqueeze(0).cuda()\n",
    "        text_attention_mask = torch.ones(len(text_ids[0])).unsqueeze(0).cuda()\n",
    "        \n",
    "        img_v = model.vision_model(image)\n",
    "        text_v = model.text_model(text_ids, text_attention_mask)\n",
    "\n",
    "        # img_v = model.image_proj(img_v) # batch_size, dim\n",
    "        # text_v = model.text_proj(text_v) # batch_size, dim\n",
    "\n",
    "        # normalize for cosine similarity\n",
    "        img_v = F.normalize(img_v, dim=1)\n",
    "        text_v = F.normalize(text_v, dim=1)\n",
    "\n",
    "        all_img_v.append(img_v)\n",
    "        all_text_v.append(text_v)\n",
    "        data.append({\n",
    "            'x': image.cpu().numpy(),\n",
    "            'y': tokenizer.decode(text_ids.squeeze(), skip_special_tokens=True),\n",
    "            'x_embed': img_v.squeeze().cpu().numpy(),\n",
    "            'y_embed': text_v.squeeze().cpu().numpy()\n",
    "        })"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]/tmp/user/21203/ipykernel_1032840/3467986279.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  image = torch.tensor(image).unsqueeze(0).cuda()\n",
      "100%|██████████| 10000/10000 [25:23<00:00,  6.56it/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "import pickle\n",
    "with open('/pasteur/u/esui/data/c3/data_medclip_no_aug_10k.pkl', 'wb') as f:\n",
    "    pickle.dump(data, f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.15 64-bit ('medclip': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "interpreter": {
   "hash": "61857b25396b5fdce226e86972ac135f7c25adc6dba8b82f788ba1e55ec75b38"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}