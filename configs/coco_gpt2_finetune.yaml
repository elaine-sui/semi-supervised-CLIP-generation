experiment_name: 'clip_prefix_cap_coco_gpt2_finetune'

lightning:
  trainer:
    gpus: 1
    #distributed_backend: 'ddp'
    max_epochs: 10
    lr: 2e-5
    precision: 32
    auto_lr_find: false
  checkpoint_callback:
    monitor: 'val/train_loss_epoch'
    dirpath: '/pasteur/u/esui/data/coco/gpt2_finetune_ckpts'
    save_last: true
    mode: 'min'
    save_top_k: 10
  # early_stopping_callback:
  #   monitor: 'val/pretrain_loss_epoch'
  #   min_delta: 0.00
  #   patience: 200
  #   verbose: False
  #   mode: 'min'
  logger:
    logger_type: 'WandbLogger'
    save_dir: '/pasteur/u/esui/data/logger/'
    name: 'clip_prefix_cap_coco'
    project: 'clip_prefix_cap_coco'


model: 
  num_layers: 8
  mapping_type: 'mlp'
  only_prefix: False
  prefix_length: 10
  clip_size: 10
  normalize_prefix: False
  is_rn: False

data: 
  dataset: 'coco'
  train_data_path: '/pasteur/u/esui/data/coco/oscar_split_train.pkl'
  val_data_path: '/pasteur/u/esui/data/coco/oscar_split_val.pkl'
  test_data_path: '/pasteur/u/esui/data/coco/oscar_split_test.pkl'
  out_dir: '/pasteur/u/esui/data/coco/gpt2_finetune_ckpts'
    
train: 
  batch_size: 40
  num_workers: 8
  warmup_epochs: 10
  loss_fn: 
    name: 'cross_entropy'
  optimizer: 
    name: 'AdamW'

